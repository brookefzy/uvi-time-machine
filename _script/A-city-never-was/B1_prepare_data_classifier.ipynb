{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Put the GSV files into train and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from fiona.crs import from_epsg\n",
    "import os\n",
    "from scipy import spatial\n",
    "import shutil\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import random\n",
    "import shapely\n",
    "\n",
    "import multiprocessing as mp\n",
    "from haversine import haversine, Unit\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import h3\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.gsvload import GSVSummary as gsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloudapi = \"AIzaSyCohhLdvyTC0UsGriQ9j-rU8pRln5wVVG8\"\n",
    "serviceaccount = \"../../google_drive_personal.json\"\n",
    "import gspread\n",
    "# from oauth2client.service_account import ServiceAccountCredentials\n",
    "gc = gspread.service_account(filename = serviceaccount)\n",
    "def read_url(url, SHEET_NAME):\n",
    "    SHEET_ID = url.split('/')[5]\n",
    "    spreadsheet = gc.open_by_key(SHEET_ID)\n",
    "    worksheet = spreadsheet.worksheet(SHEET_NAME)\n",
    "    rows = worksheet.get_all_records()\n",
    "    df_spread = pd.DataFrame(rows)\n",
    "    return df_spread, worksheet\n",
    "\n",
    "url = \"https://docs.google.com/spreadsheets/d/1o5gFmZPUoDwrrbfE6M26uJF3HnEZll02ivnOxP6K6Xw/edit?usp=sharing\"\n",
    "SHEETNAME = \"select_city\"\n",
    "city_meta, other_worksheet = read_url( url, SHEETNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_meta_1 = city_meta[city_meta['City']!='Gaborone'].reset_index(drop=True)\n",
    "# city_meta_2 = city_meta[city_meta['City']=='Gaborone'].reset_index(drop=True)\n",
    "# city_meta_1['label'] = city_meta_1.index\n",
    "# city_meta_2['label'] = 34\n",
    "# city_meta = pd.concat([city_meta_1, city_meta_2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_meta_label = dict(zip(city_meta['City'], city_meta['label']))\n",
    "city_meta_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_worksheet.update([city_meta.columns.values.tolist()] + city_meta.values.tolist()) # save the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Boston\"\n",
    "cityabbrlower = city.lower().replace(\" \", \"\")\n",
    "citysummary = gsv(city)\n",
    "selmeta = citysummary.merge_meta()\n",
    "selmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size threshold = 15000\n",
    "sthres = 15000\n",
    "dis_thred = 30000\n",
    "total_h3_8 = 3000\n",
    "selmeta = selmeta[(selmeta['size']>=sthres)&(selmeta['dist_hav']<=30000)].reset_index(drop=True)\n",
    "# train_sel = selmeta[selmeta['data_group']=='train'].sample(n=20000, random_state=1)\n",
    "# test_sel = selmeta[selmeta['data_group']=='test'].sample(n=1000, random_state=1)\n",
    "# val_sel = selmeta[selmeta['data_group']=='val'].sample(n=1000, random_state=1)\n",
    "h3summary = selmeta.groupby('h3_res8').size().reset_index(name='counts').sort_values(by='counts', ascending=True)\n",
    "h3summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many h3_8 for each city exist within the size and distance threshold\n",
    "# within each h3_8, we choose 80% (h3_9) for training, 10% for testing, 10% for validation.\n",
    "# However, need to avoid unbalanced data, so we need to check the number of h3_9 for each city\n",
    "# h3 count\n",
    "# h3_9 count\n",
    "h3_9_count = []\n",
    "h3_8_count = []\n",
    "\n",
    "for city in tqdm(list(city_meta['City'].unique())[:]):\n",
    "    cityabbrlower = city.lower().replace(\" \", \"\")\n",
    "    citysummary = gsv(city)\n",
    "    selmeta = citysummary.merge_meta(sel = False)\n",
    "    sthres = 15000\n",
    "    dis_thred = 30000\n",
    "    \n",
    "    center_lat = city_meta[city_meta['City']==city]['center_lat'].values[0]\n",
    "    center_lng = city_meta[city_meta['City']==city]['center_lng'].values[0]\n",
    "\n",
    "    # get distance to center\n",
    "    selmeta['dist_hav'] = selmeta.apply(lambda row: haversine((row['lat'], row['lon']), (center_lat, center_lng),\n",
    "                                                            unit = 'm'), axis=1)\n",
    "    \n",
    "    # selmeta = selmeta[(selmeta['size']>=sthres)&(selmeta['dist_hav']<=30000)].reset_index(drop=True)\n",
    "    \n",
    "    for res in [8, 9]:\n",
    "        selmeta[f'h3_res{res}'] = selmeta.apply(lambda row: h3.geo_to_h3(row['lat'], row['lon'], res), axis=1)\n",
    "    h3_9_count.append(selmeta['h3_res9'].nunique())\n",
    "    h3_8_count.append(selmeta['h3_res8'].nunique())\n",
    "    selmeta.to_csv(os.path.join(citysummary.metafolder, f'{cityabbrlower}_meta.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_meta['h3_9_count'] = h3_9_count\n",
    "city_meta['h3_8_count'] = h3_8_count\n",
    "city_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within each h3 8, select 5 h3 9 for training, 1 for testing, 1 for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_worksheet.update([city_meta.columns.values.tolist()] + city_meta.values.tolist()) # save the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel = selmeta[selmeta['data_group']=='train'].groupby('h3_res9').sample(n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labeled df\n",
    "\n",
    "trainset = []\n",
    "\n",
    "for city in tqdm(list(city_meta['City'].unique())[7:]):\n",
    "    cityabbrlower = city.lower().replace(\" \", \"\")\n",
    "    citysummary = gsv(city)\n",
    "    selmeta = citysummary.merge_meta()\n",
    "    # random select 20000 images from train, 2000 from test, 2000 from val\n",
    "    selmeta['label'] = city\n",
    "    # selmeta.rename(columns = {'data_group':'test'}, inplace = True)\n",
    "    selmeta = selmeta[['path','label', 'data_group']]\n",
    "    if selmeta.shape[0]>=24000:\n",
    "        train_sel = selmeta[selmeta['data_group']=='train'].sample(n=20000, random_state=1)\n",
    "        test_sel = selmeta[selmeta['data_group']=='test'].sample(n=1000, random_state=1)\n",
    "        val_sel = selmeta[selmeta['data_group']=='val'].sample(n=1000, random_state=1)\n",
    "        trainset.append(train_sel)\n",
    "        trainset.append(test_sel)\n",
    "        trainset.append(val_sel)\n",
    "    else:\n",
    "        print(\"City with fewer than 24000 images: \", city)\n",
    "        continue\n",
    "\n",
    "    \n",
    "trainset = pd.concat(trainset)\n",
    "\n",
    "# make a folder under train, test val for the city\n",
    "folder_train = os.path.join(train_path, cityabbrlower)\n",
    "folder_test = os.path.join(test_path, cityabbrlower)\n",
    "folder_val = os.path.join(val_path, cityabbrlower)\n",
    "if not os.path.exists(folder_train):\n",
    "    os.makedirs(folder_train)\n",
    "if not os.path.exists(folder_test):\n",
    "    os.makedirs(folder_test)\n",
    "if not os.path.exists(folder_val):\n",
    "    os.makedirs(folder_val)\n",
    "# copy the images to the folder\n",
    "for i, row in train_sel.iterrows():\n",
    "    shutil.copy(row['image_path'], folder_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'] = df['path'].apply(lambda x: x.replace(\"./data/\", \"/host_dir/08_GSV/data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign train test val\n",
    "df = pd.read_csv(\"./data/classifier_trainset_update.csv\")\n",
    "df['path'] = df['path'].apply(lambda x: x.replace(\"./data/\", \"/host_dir/08_GSV/data/\"))\n",
    "print(\"original: \", df.shape[0])\n",
    "train = df.groupby(['label','city']).apply(lambda x: x.sample(n=1500, random_state=1)).reset_index(drop = True)\n",
    "\n",
    "print(\"train: \", train.shape[0])\n",
    "remain = df[~df.path.isin(train.path)].reset_index(drop = True)\n",
    "print(\"remain: \",remain.shape[0])\n",
    "test = remain.groupby(['label','city']).apply(lambda x: x.sample(frac=0.5, random_state=1)).reset_index(drop = True)\n",
    "print(test.shape[0])\n",
    "val = remain[~remain.path.isin(test.path)].reset_index(drop = True)\n",
    "print(val.shape[0])\n",
    "train['data_group'] = 'train'\n",
    "test['data_group'] = 'test'\n",
    "val['data_group'] = 'val'\n",
    "df = pd.concat([train, test, val])\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset['city'] = trainset['label']\n",
    "# trainset['label'] = trainset['city'].apply(lambda x: city_meta_label[x])\n",
    "df.to_csv(\"./data/classifier_trainset_update_1.csv\", index = False)\n",
    "# trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>data_group</th>\n",
       "      <th>city</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/gsv_rgb/accra/img_rgb/4_1/7/f/IJ--mMcyT...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>Accra</td>\n",
       "      <td>37960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/gsv_rgb/accra/img_rgb/b_1/1/8/-tP18xo2B...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>Accra</td>\n",
       "      <td>32807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/gsv_rgb/accra/img_rgb/4_1/4/4/ApH9sVZH3...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>Accra</td>\n",
       "      <td>32206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/gsv_rgb/accra/img_rgb/5_1/e/d/kgodz5sI0...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>Accra</td>\n",
       "      <td>42600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/gsv_rgb/accra/img_rgb/9_1/3/1/LDUY2d_O4...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>Accra</td>\n",
       "      <td>32506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label data_group   city   \n",
       "0  ./data/gsv_rgb/accra/img_rgb/4_1/7/f/IJ--mMcyT...     11      train  Accra  \\\n",
       "1  ./data/gsv_rgb/accra/img_rgb/b_1/1/8/-tP18xo2B...     11      train  Accra   \n",
       "2  ./data/gsv_rgb/accra/img_rgb/4_1/4/4/ApH9sVZH3...     11      train  Accra   \n",
       "3  ./data/gsv_rgb/accra/img_rgb/5_1/e/d/kgodz5sI0...     11      train  Accra   \n",
       "4  ./data/gsv_rgb/accra/img_rgb/9_1/3/1/LDUY2d_O4...     11      train  Accra   \n",
       "\n",
       "    size  \n",
       "0  37960  \n",
       "1  32807  \n",
       "2  32206  \n",
       "3  42600  \n",
       "4  32506  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/lustre1/g/geog_pyloo/05_timemachine/GSV/classifier_trainset_update.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo311",
   "language": "python",
   "name": "geo311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
