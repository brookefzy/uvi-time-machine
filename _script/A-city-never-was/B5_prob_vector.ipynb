{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime\n",
    "import argparse\n",
    "import h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ROOTFOLDER = \"/lustre1/g/geog_pyloo/05_timemachine\"\n",
    "VALFOLDER = (\n",
    "    \"/lustre1/g/geog_pyloo/05_timemachine/_transformed/t_classifier_img_yolo8_inf_dir\"\n",
    ")\n",
    "CURATED_FOLDER = (\n",
    "    \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob\"\n",
    ")\n",
    "TRAIN_TEST_FOLDER = \"/lustre1/g/geog_pyloo/05_timemachine/_transformed/t_classifier_img_yolo8\"\n",
    "RAW_PATH = \"/lustre1/g/geog_pyloo/05_timemachine/GSV/gsv_rgb/{city}/gsvmeta/{city}_meta.csv\"\n",
    "\n",
    "PANO_PATH = \"{ROOTFOLDER}/GSV/gsv_rgb/{cityabbr}/gsvmeta/gsv_pano.csv\"\n",
    "PATH_PATH = \"{ROOTFOLDER}/GSV/gsv_rgb/{cityabbr}/gsvmeta/gsv_path.csv\"\n",
    "\n",
    "CURATE_FOLDER_SOURCE = \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob_hex_summary\"\n",
    "CURATE_FOLDER_EXPORT = \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob_similarity\"\n",
    "\n",
    "if not os.path.exists(CURATE_FOLDER_EXPORT):\n",
    "    os.makedirs(CURATE_FOLDER_EXPORT)\n",
    "    \n",
    "vector_ls = [str(x) for x in range(0, 127)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scripts for batch processing\n",
    "lines = \"\"\"python /home/yuanzf/uvi-time-machine/_script/A-city-never-was/B5_prob_vector_summary.py --city {city}\"\"\"\n",
    "city_meta = pd.read_csv(\"/home/yuanzf/uvi-time-machine/_script/city_meta.csv\")\n",
    "city_ls = city_meta.City.values\n",
    "# split the cities into four groups to run the script in parallel\n",
    "N = len(city_ls) // 10\n",
    "for i in range(N):\n",
    "    with open(f\"run_b5_{i}.sh\", \"w\") as f:\n",
    "        for city in city_ls[i*10:(i+1)*10]:\n",
    "            f.write(lines.format(city=city) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct similarity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the similarity matrix among all cities\n",
    "# load the results first\n",
    "# compute the similarity matrix among all cells\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc\n",
    "\n",
    "RES_EXCLUDE = 11\n",
    "\n",
    "# OUTPUT_FILE_NAME = \"prob_city={city}_res_exclude={res_exclude}.parquet\"\n",
    "def load_all(res_sel,\n",
    "             res_exclude=RES_EXCLUDE,\n",
    "             ):\n",
    "    files = glob(CURATE_FOLDER_SOURCE + f\"/*res_exclude={res_exclude}.parquet\")\n",
    "    print(len(files))\n",
    "    df_all = []\n",
    "    for f in files:\n",
    "        temp = pd.read_parquet(f)\n",
    "        temp = temp[temp.res == res_sel].reset_index(drop = True)\n",
    "        temp['city'] = os.path.basename(f).split(\"_\")[1].replace(\"city=\", \"\")\n",
    "        df_all.append(temp)\n",
    "    df_all = pd.concat(df_all).drop_duplicates('hex_id').reset_index(drop = True)\n",
    "    df_all = df_all.drop(columns = [\"res\"])\n",
    "    print(\"Data loaded\", df_all.shape[0])\n",
    "    n_cells = df_all.shape[0]\n",
    "    X = df_all[vector_ls].values\n",
    "    # create a new dataframe that has shape of (n_cells, 2) to store the similarity matrix\n",
    "    # compute the similarity matrix\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    print(\"Similarity matrix computed\", similarity_matrix.shape)\n",
    "\n",
    "    # only keep the upper triangle of the matrix\n",
    "    similarity_matrix = np.triu(similarity_matrix, k=1)\n",
    "    print(\"Upper triangle extracted\", similarity_matrix.shape)\n",
    "    \n",
    "    gc.collect()\n",
    "    hex_ls = df_all.hex_id.values\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index = hex_ls, columns = hex_ls)\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.stack()\n",
    "    similarity_df = pd.DataFrame(similarity_df).reset_index()\n",
    "    similarity_df.columns = [\"hex_id1\", \"hex_id2\", \"similarity\"]\n",
    "\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = 'hex_id1', right_on = 'hex_id')\\\n",
    "        .drop([\"hex_id\"], axis = 1)\\\n",
    "        .merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = \"hex_id2\", right_on = 'hex_id', suffixes = [\"_1\", \"_2\"])\\\n",
    "            .drop([\"hex_id\"], axis = 1)\n",
    "    for city in city_ls:\n",
    "        temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "        temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "    print(\"Similarity df saved city by city\")\n",
    "\n",
    "    summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "    summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))\n",
    "    \n",
    "    return summarydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing res_sel=7\n",
      "112\n",
      "Data loaded 23886\n",
      "Similarity matrix computed (23886, 23886)\n",
      "Upper triangle extracted (23886, 23886)\n",
      "Similarity df saved city by city\n"
     ]
    }
   ],
   "source": [
    "# convert the similarity matrix to a dataframe\n",
    "res_exclude = 11\n",
    "for res_sel in [7]:\n",
    "    print(f\"Processing res_sel={res_sel}\")\n",
    "    files = glob(CURATE_FOLDER_SOURCE + f\"/*res_exclude={res_exclude}.parquet\")\n",
    "    print(len(files))\n",
    "    df_all = []\n",
    "    for f in files:\n",
    "        temp = pd.read_parquet(f)\n",
    "        temp = temp[temp.res == res_sel].reset_index(drop = True)\n",
    "        temp['city'] = os.path.basename(f).split(\"_\")[1].replace(\"city=\", \"\")\n",
    "        df_all.append(temp)\n",
    "    df_all = pd.concat(df_all).drop_duplicates('hex_id').reset_index(drop = True)\n",
    "    df_all = df_all.drop(columns = [\"res\"])\n",
    "    print(\"Data loaded\", df_all.shape[0])\n",
    "    n_cells = df_all.shape[0]\n",
    "    X = df_all[vector_ls].values\n",
    "    # create a new dataframe that has shape of (n_cells, 2) to store the similarity matrix\n",
    "    # compute the similarity matrix\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    print(\"Similarity matrix computed\", similarity_matrix.shape)\n",
    "\n",
    "    # only keep the upper triangle of the matrix\n",
    "    similarity_matrix = np.triu(similarity_matrix, k=1)\n",
    "    print(\"Upper triangle extracted\", similarity_matrix.shape)\n",
    "    \n",
    "    gc.collect()\n",
    "    hex_ls = df_all.hex_id.values\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index = hex_ls, columns = hex_ls)\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.stack()\n",
    "    similarity_df = pd.DataFrame(similarity_df).reset_index()\n",
    "    similarity_df.columns = [\"hex_id1\", \"hex_id2\", \"similarity\"]\n",
    "\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = 'hex_id1', right_on = 'hex_id')\\\n",
    "        .drop([\"hex_id\"], axis = 1)\\\n",
    "        .merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = \"hex_id2\", right_on = 'hex_id', suffixes = [\"_1\", \"_2\"])\\\n",
    "            .drop([\"hex_id\"], axis = 1)\n",
    "    city_ls = df_all.city.unique()\n",
    "    for city in city_ls:\n",
    "        temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "        temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "    print(\"Similarity df saved city by city\")\n",
    "\n",
    "    summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "    summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity df saved city by city\n"
     ]
    }
   ],
   "source": [
    "city_ls = df_all.city.unique()\n",
    "for city in city_ls:\n",
    "    temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "    temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "print(\"Similarity df saved city by city\")\n",
    "\n",
    "summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 33.333333333333336\n",
      "2 40.0\n",
      "3 42.85714285714286\n",
      "4 33.333333333333336\n",
      "5 42.85714285714286\n",
      "6 60.0\n",
      "7 37.5\n",
      "8 25.0\n",
      "9 60.0\n",
      "10 28.571428571428573\n",
      "11 60.0\n",
      "42.13203463203463\n"
     ]
    }
   ],
   "source": [
    "n_author = 3\n",
    "is_first = True\n",
    "def get_percentage(n_author = 3, is_first=True):\n",
    "    x = 100/(3+n_author)\n",
    "    if is_first:\n",
    "        return 3*x\n",
    "    else:\n",
    "        return 2*x\n",
    "all_paper = {\n",
    "    1:[3, False],\n",
    "    2:[2, False],\n",
    "    3:[4, True],\n",
    "    4:[3, False],\n",
    "    5:[4, True],\n",
    "    6:[2, True],\n",
    "    7: [5, True],\n",
    "    8: [5, False],\n",
    "    9: [2, True],\n",
    "    10:[4, False],\n",
    "    11:[2, True]\n",
    "}\n",
    "result = []\n",
    "for i in all_paper.keys():\n",
    "    print(i, get_percentage(all_paper[i][0], all_paper[i][1]))\n",
    "    result.append(get_percentage(all_paper[i][0], all_paper[i][1]))\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
