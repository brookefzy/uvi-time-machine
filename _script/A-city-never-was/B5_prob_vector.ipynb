{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime\n",
    "import argparse\n",
    "import h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ROOTFOLDER = \"/lustre1/g/geog_pyloo/05_timemachine\"\n",
    "VALFOLDER = (\n",
    "    \"/lustre1/g/geog_pyloo/05_timemachine/_transformed/t_classifier_img_yolo8_inf_dir\"\n",
    ")\n",
    "CURATED_FOLDER = (\n",
    "    \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob\"\n",
    ")\n",
    "TRAIN_TEST_FOLDER = \"/lustre1/g/geog_pyloo/05_timemachine/_transformed/t_classifier_img_yolo8\"\n",
    "RAW_PATH = \"/lustre1/g/geog_pyloo/05_timemachine/GSV/gsv_rgb/{city}/gsvmeta/{city}_meta.csv\"\n",
    "\n",
    "PANO_PATH = \"{ROOTFOLDER}/GSV/gsv_rgb/{cityabbr}/gsvmeta/gsv_pano.csv\"\n",
    "PATH_PATH = \"{ROOTFOLDER}/GSV/gsv_rgb/{cityabbr}/gsvmeta/gsv_path.csv\"\n",
    "\n",
    "CURATE_FOLDER_SOURCE = \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob_hex_summary\"\n",
    "CURATE_FOLDER_EXPORT = \"/lustre1/g/geog_pyloo/05_timemachine/_curated/c_city_classifiier_prob_similarity\"\n",
    "\n",
    "if not os.path.exists(CURATE_FOLDER_EXPORT):\n",
    "    os.makedirs(CURATE_FOLDER_EXPORT)\n",
    "    \n",
    "vector_ls = [str(x) for x in range(0, 127)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scripts for batch processing\n",
    "lines = \"\"\"python /home/yuanzf/uvi-time-machine/_script/A-city-never-was/B5_prob_vector_summary.py --city {city}\"\"\"\n",
    "city_meta = pd.read_csv(\"/home/yuanzf/uvi-time-machine/_script/city_meta.csv\")\n",
    "city_ls = city_meta.City.values\n",
    "# split the cities into four groups to run the script in parallel\n",
    "N = len(city_ls) // 10\n",
    "for i in range(N):\n",
    "    with open(f\"run_b5_{i}.sh\", \"w\") as f:\n",
    "        for city in city_ls[i*10:(i+1)*10]:\n",
    "            f.write(lines.format(city=city) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct similarity indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the similarity matrix among all cities\n",
    "# load the results first\n",
    "# compute the similarity matrix among all cells\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc\n",
    "\n",
    "RES_EXCLUDE = 11\n",
    "\n",
    "# OUTPUT_FILE_NAME = \"prob_city={city}_res_exclude={res_exclude}.parquet\"\n",
    "def load_all(res_sel,\n",
    "             res_exclude=RES_EXCLUDE,\n",
    "             ):\n",
    "    files = glob(CURATE_FOLDER_SOURCE + f\"/*res_exclude={res_exclude}.parquet\")\n",
    "    print(len(files))\n",
    "    df_all = []\n",
    "    for f in files:\n",
    "        temp = pd.read_parquet(f)\n",
    "        temp = temp[temp.res == res_sel].reset_index(drop = True)\n",
    "        temp['city'] = os.path.basename(f).split(\"_\")[1].replace(\"city=\", \"\")\n",
    "        df_all.append(temp)\n",
    "    df_all = pd.concat(df_all).drop_duplicates('hex_id').reset_index(drop = True)\n",
    "    df_all = df_all.drop(columns = [\"res\"])\n",
    "    print(\"Data loaded\", df_all.shape[0])\n",
    "    n_cells = df_all.shape[0]\n",
    "    X = df_all[vector_ls].values\n",
    "    # create a new dataframe that has shape of (n_cells, 2) to store the similarity matrix\n",
    "    # compute the similarity matrix\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    print(\"Similarity matrix computed\", similarity_matrix.shape)\n",
    "\n",
    "    # only keep the upper triangle of the matrix\n",
    "    similarity_matrix = np.triu(similarity_matrix, k=1)\n",
    "    print(\"Upper triangle extracted\", similarity_matrix.shape)\n",
    "    \n",
    "    gc.collect()\n",
    "    hex_ls = df_all.hex_id.values\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index = hex_ls, columns = hex_ls)\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.stack()\n",
    "    similarity_df = pd.DataFrame(similarity_df).reset_index()\n",
    "    similarity_df.columns = [\"hex_id1\", \"hex_id2\", \"similarity\"]\n",
    "\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = 'hex_id1', right_on = 'hex_id')\\\n",
    "        .drop([\"hex_id\"], axis = 1)\\\n",
    "        .merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = \"hex_id2\", right_on = 'hex_id', suffixes = [\"_1\", \"_2\"])\\\n",
    "            .drop([\"hex_id\"], axis = 1)\n",
    "    for city in city_ls:\n",
    "        temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "        temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "    print(\"Similarity df saved city by city\")\n",
    "\n",
    "    summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "    summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))\n",
    "    \n",
    "    return summarydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_city_pair_ls():\n",
    "    city_meta = pd.read_csv(\"/home/yuanzf/uvi-time-machine/_script/city_meta.csv\")\n",
    "    city_ls = city_meta.City.values\n",
    "    pair_ls = np.array(np.meshgrid(city_ls, city_ls)).T.reshape(-1, 2)\n",
    "    pair_ls = pair_ls[pair_ls[:, 0] != pair_ls[:, 1]]  # exclude same city\n",
    "    return pair_ls\n",
    "\n",
    "pair_ls = create_city_pair_ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16002, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_ls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing res_sel=6\n",
      "2\n",
      "Data loaded 73\n",
      "Similarity matrix computed (73, 73)\n",
      "Upper triangle extracted (73, 73)\n"
     ]
    }
   ],
   "source": [
    "# convert the similarity matrix to a dataframe\n",
    "res_exclude = 11\n",
    "city_1 = \"Hong Kong\"\n",
    "city_2 = \"Sheffield\"\n",
    "filename = \"prob_city={city_name}_res_exclude={res_exclude}.parquet\"\n",
    "\n",
    "for res_sel in [6]:\n",
    "    print(f\"Processing res_sel={res_sel}\")\n",
    "    files = glob(\n",
    "        CURATE_FOLDER_SOURCE\n",
    "        + \"/\"\n",
    "        + filename.format(city_name=city_1, res_exclude=res_exclude)\n",
    "    ) + glob(\n",
    "        CURATE_FOLDER_SOURCE\n",
    "        + \"/\"\n",
    "        + filename.format(city_name=city_2, res_exclude=res_exclude)\n",
    "    )\n",
    "    print(len(files))\n",
    "    df_all = []\n",
    "    for f in files:\n",
    "        temp = pd.read_parquet(f)\n",
    "        temp = temp[temp.res == res_sel].reset_index(drop = True)\n",
    "        temp['city'] = os.path.basename(f).split(\"_\")[1].replace(\"city=\", \"\")\n",
    "        df_all.append(temp)\n",
    "    df_all = pd.concat(df_all).drop_duplicates('hex_id').reset_index(drop = True)\n",
    "    df_all = df_all.drop(columns = [\"res\"])\n",
    "    print(\"Data loaded\", df_all.shape[0])\n",
    "    n_cells = df_all.shape[0]\n",
    "    X = df_all[vector_ls].values\n",
    "    # create a new dataframe that has shape of (n_cells, 2) to store the similarity matrix\n",
    "    # compute the similarity matrix\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    print(\"Similarity matrix computed\", similarity_matrix.shape)\n",
    "\n",
    "    # only keep the upper triangle of the matrix\n",
    "    similarity_matrix = np.triu(similarity_matrix, k=1)\n",
    "    print(\"Upper triangle extracted\", similarity_matrix.shape)\n",
    "\n",
    "    gc.collect()\n",
    "    hex_ls = df_all.hex_id.values\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index = hex_ls, columns = hex_ls)\n",
    "    gc.collect()\n",
    "    similarity_df = similarity_df.stack()\n",
    "    similarity_df = pd.DataFrame(similarity_df).reset_index()\n",
    "    similarity_df.columns = [\"hex_id1\", \"hex_id2\", \"similarity\"]\n",
    "\n",
    "    gc.collect()\n",
    "    # similarity_df = similarity_df.merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = 'hex_id1', right_on = 'hex_id')\\\n",
    "    #     .drop([\"hex_id\"], axis = 1)\\\n",
    "    #     .merge(df_all[['hex_id', 'city']].drop_duplicates(), left_on = \"hex_id2\", right_on = 'hex_id', suffixes = [\"_1\", \"_2\"])\\\n",
    "    #         .drop([\"hex_id\"], axis = 1)\n",
    "    # city_ls = df_all.city.unique()\n",
    "    # for city in city_ls:\n",
    "    #     temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "    #     temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "    # print(\"Similarity df saved city by city\")\n",
    "\n",
    "    # summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "    # summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hex_id1</th>\n",
       "      <th>hex_id2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420cfffffff</td>\n",
       "      <td>0.005752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hex_id1          hex_id2  similarity\n",
       "57  864103487ffffff  8619420cfffffff    0.005752"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df[(similarity_df[\"hex_id1\"] == hex_1) & (similarity_df[\"hex_id2\"] == hex_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity df saved city by city\n"
     ]
    }
   ],
   "source": [
    "city_ls = df_all.city.unique()\n",
    "for city in city_ls:\n",
    "    temp = similarity_df[(similarity_df.city_1 == city)]\n",
    "    temp.to_parquet(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_connection_res={res_sel}_city={city}.parquet'))\n",
    "print(\"Similarity df saved city by city\")\n",
    "\n",
    "summarydf = similarity_df.groupby(['city_1', 'city_2']).size().reset_index()\n",
    "summarydf.to_csv(os.path.join(CURATE_FOLDER_EXPORT, f'similarity_summary_connection_res={res_sel}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC.1. Why two vectors have 0 cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Hong Kong\"\n",
    "res_sel = 6\n",
    "temp = pd.read_parquet(\n",
    "    os.path.join(\n",
    "        CURATE_FOLDER_EXPORT, f\"similarity_connection_res={res_sel}_city={city}.parquet\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hex_id1</th>\n",
       "      <th>hex_id2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420c7ffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420cfffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420d7ffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420dfffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864103487ffffff</td>\n",
       "      <td>8619420e7ffffff</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Sheffield</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hex_id1          hex_id2  similarity     city_1     city_2\n",
       "0  864103487ffffff  8619420c7ffffff         0.0  Hong Kong  Sheffield\n",
       "1  864103487ffffff  8619420cfffffff         0.0  Hong Kong  Sheffield\n",
       "2  864103487ffffff  8619420d7ffffff         0.0  Hong Kong  Sheffield\n",
       "3  864103487ffffff  8619420dfffffff         0.0  Hong Kong  Sheffield\n",
       "4  864103487ffffff  8619420e7ffffff         0.0  Hong Kong  Sheffield"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = temp.reset_index(drop=True)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_1 = '864103487ffffff'\n",
    "hex_2 = \"8619420cfffffff\"\n",
    "city_1 = \"Hong Kong\"\n",
    "city_2 = \"Sheffield\"\n",
    "RES_EXCLUDE = 11\n",
    "filename = \"prob_city={city_name}_res_exclude={res_exclude}.parquet\"\n",
    "# load the probablity vectors for the two hexagons\n",
    "temp1 = pd.read_parquet(\n",
    "    os.path.join(\n",
    "        CURATE_FOLDER_SOURCE,\n",
    "        filename.format(city_name=city_1, res_exclude=RES_EXCLUDE),\n",
    "    )\n",
    ")\n",
    "vec1 = temp1[temp1.hex_id == hex_1][vector_ls].values[0]\n",
    "temp2 = pd.read_parquet(\n",
    "    os.path.join(\n",
    "        CURATE_FOLDER_SOURCE,\n",
    "        filename.format(city_name=city_2, res_exclude=RES_EXCLUDE),\n",
    "    )\n",
    ")\n",
    "vec2 = temp2[temp2.hex_id == hex_2][vector_ls].values[0]\n",
    "# compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00575179]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual check the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([vec2], [vec1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC.2. Why the cosine similarity is asymetrical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
